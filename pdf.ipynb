{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08faba07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 測試單一 (Debug)\n",
      "2. 執行全部 (邊跑邊存)\n",
      "開始處理，結果將即時儲存至: final_corrected_data_incremental.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [36:47<00:00, 44.15s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "全數完成！\n",
      "結果檔案: C:\\Users\\wesley\\OneDrive\\桌面\\LAB\\ai_cup\\company_data_by_annotation_group\\all\\final_corrected_data_incremental.csv\n",
      "錯誤紀錄: C:\\Users\\wesley\\OneDrive\\桌面\\LAB\\ai_cup\\company_data_by_annotation_group\\all\\processing_errors_incremental.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "import pdfplumber\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 設定工作目錄\n",
    "TARGET_DIR = r\"C:\\Users\\wesley\\OneDrive\\桌面\\LAB\\ai_cup\\company_data_by_annotation_group\\all\"\n",
    "\n",
    "# 設定輸出檔名\n",
    "OUTPUT_CSV = \"final_corrected_data_incremental.csv\"\n",
    "ERROR_CSV = \"processing_errors_incremental.csv\"\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"清洗文字\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return re.sub(r'[^\\w\\u4e00-\\u9fff]', '', text).lower()\n",
    "\n",
    "def get_shingles(text, n=2):\n",
    "    \"\"\"切分 n-gram\"\"\"\n",
    "    if len(text) < n:\n",
    "        return {text}\n",
    "    return set(text[i:i+n] for i in range(len(text) - n + 1))\n",
    "\n",
    "def calculate_hybrid_score(target, page_content):\n",
    "    \"\"\"混合評分機制 (Chunking + Shingling)\"\"\"\n",
    "    if not target or not page_content:\n",
    "        return 0.0\n",
    "    \n",
    "    # 策略 1: 連續片段\n",
    "    chunk_size = 10\n",
    "    chunks = [target[i:i+chunk_size] for i in range(0, len(target), chunk_size) if len(target[i:i+chunk_size]) > 4]\n",
    "    \n",
    "    chunk_hits = 0\n",
    "    if chunks:\n",
    "        for chunk in chunks:\n",
    "            if chunk in page_content:\n",
    "                chunk_hits += 1\n",
    "        chunk_score = chunk_hits / len(chunks)\n",
    "    else:\n",
    "        chunk_score = 0.0\n",
    "\n",
    "    if chunk_score > 0.6:\n",
    "        return chunk_score\n",
    "\n",
    "    # 策略 2: 碎片集合\n",
    "    target_shingles = get_shingles(target, n=2)\n",
    "    if not target_shingles:\n",
    "        return 0.0\n",
    "        \n",
    "    hit_count = 0\n",
    "    for s in target_shingles:\n",
    "        if s in page_content:\n",
    "            hit_count += 1\n",
    "            \n",
    "    shingle_score = hit_count / len(target_shingles)\n",
    "    \n",
    "    return max(chunk_score, shingle_score)\n",
    "\n",
    "def split_pdf(file_path, company_code_stock):\n",
    "    \"\"\"切分 PDF\"\"\"\n",
    "    reader = PdfReader(file_path)\n",
    "    base_dir = os.path.dirname(file_path)\n",
    "    \n",
    "    check_file = os.path.join(base_dir, f\"{company_code_stock}_page_1.pdf\")\n",
    "    if os.path.exists(check_file):\n",
    "        return len(reader.pages)\n",
    "\n",
    "    # 增加錯誤處理，避免硬碟滿了程式崩潰\n",
    "    try:\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            writer = PdfWriter()\n",
    "            writer.add_page(page)\n",
    "            output_path = os.path.join(base_dir, f\"{company_code_stock}_page_{i+1}.pdf\")\n",
    "            with open(output_path, \"wb\") as f:\n",
    "                writer.write(f)\n",
    "        return len(reader.pages)\n",
    "    except OSError:\n",
    "        print(f\"警告：切分 {company_code_stock} 時空間不足或發生錯誤。\")\n",
    "        return 0\n",
    "\n",
    "def process_single_folder(folder_path, folder_name, is_test_mode=False):\n",
    "    pdf_files = glob.glob(os.path.join(folder_path, \"*.pdf\"))\n",
    "    original_pdf = [f for f in pdf_files if \"_page_\" not in f]\n",
    "    json_files = glob.glob(os.path.join(folder_path, \"*.json\"))\n",
    "    \n",
    "    if not original_pdf or not json_files:\n",
    "        return [], [{\"company\": folder_name, \"error\": \"Missing PDF or JSON\"}]\n",
    "\n",
    "    pdf_path = original_pdf[0]\n",
    "    json_path = json_files[0]\n",
    "    company_key = folder_name\n",
    "\n",
    "    results = []\n",
    "    errors = []\n",
    "\n",
    "    try:\n",
    "        split_pdf(pdf_path, company_key)\n",
    "        \n",
    "        page_texts = {}\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                extracted = page.extract_text() or \"\"\n",
    "                page_texts[i + 1] = clean_text(extracted)\n",
    "\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data_list = json.load(f)\n",
    "\n",
    "        if is_test_mode:\n",
    "            print(f\"處理中: {company_key}\")\n",
    "\n",
    "        for item in data_list:\n",
    "            raw_target = item.get(\"data\", \"\")\n",
    "            raw_evidence = item.get(\"evidence_string\", \"\")\n",
    "            \n",
    "            cleaned_target = clean_text(raw_target)\n",
    "            cleaned_evidence = clean_text(raw_evidence)\n",
    "            \n",
    "            best_page = None\n",
    "            best_score = 0.0\n",
    "            \n",
    "            # 比對 Data\n",
    "            if cleaned_target:\n",
    "                for p_num, p_text in page_texts.items():\n",
    "                    if len(p_text) < 50: continue \n",
    "                    score = calculate_hybrid_score(cleaned_target, p_text)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_page = p_num\n",
    "\n",
    "            # 比對 Evidence\n",
    "            if best_score < 0.85 and cleaned_evidence:\n",
    "                for p_num, p_text in page_texts.items():\n",
    "                    if len(p_text) < 50: continue\n",
    "                    score = calculate_hybrid_score(cleaned_evidence, p_text)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_page = p_num\n",
    "\n",
    "            result_row = item.copy()\n",
    "            if best_page and best_score >= 0.4:\n",
    "                result_row[\"URL\"] = f\"local_file://{company_key}_page_{best_page}.pdf\"\n",
    "                result_row[\"page_number\"] = best_page\n",
    "                result_row[\"_status\"] = \"Success\"\n",
    "                if is_test_mode:\n",
    "                    print(f\"  [成功] 頁數: {best_page} (分數: {best_score:.2f})\")\n",
    "            else:\n",
    "                result_row[\"_status\"] = \"Not Found\"\n",
    "                errors.append({\n",
    "                    \"company\": company_key,\n",
    "                    \"target\": raw_target[:30],\n",
    "                    \"score\": best_score\n",
    "                })\n",
    "                if is_test_mode:\n",
    "                    print(f\"  [失敗] 最高分僅: {best_score:.2f}\")\n",
    "\n",
    "            results.append(result_row)\n",
    "\n",
    "    except Exception as e:\n",
    "        return [], [{\"company\": company_key, \"error\": str(e)}]\n",
    "\n",
    "    return results, errors\n",
    "\n",
    "def save_chunk(data, filename):\n",
    "    \"\"\"\n",
    "    【關鍵功能】將資料寫入 CSV\n",
    "    - 如果檔案不存在：建立新檔並寫入 Header\n",
    "    - 如果檔案已存在：用 Append 模式寫入 (不寫 Header)\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    if not os.path.isfile(filename):\n",
    "        # 檔案不存在，寫入 Header\n",
    "        df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "    else:\n",
    "        # 檔案存在，追加寫入，不寫 Header\n",
    "        df.to_csv(filename, mode='a', header=False, index=False, encoding='utf-8-sig')\n",
    "\n",
    "def main():\n",
    "    if os.path.exists(TARGET_DIR):\n",
    "        os.chdir(TARGET_DIR)\n",
    "    \n",
    "    # 初始化：如果之前有舊的檔案，先刪除，避免重複追加\n",
    "    if os.path.exists(OUTPUT_CSV):\n",
    "        print(f\"刪除舊檔案: {OUTPUT_CSV}，重新開始...\")\n",
    "        os.remove(OUTPUT_CSV)\n",
    "    if os.path.exists(ERROR_CSV):\n",
    "        os.remove(ERROR_CSV)\n",
    "\n",
    "    all_folders = [d for d in os.listdir('.') if os.path.isdir(d)]\n",
    "    \n",
    "    print(\"1. 測試單一 (Debug)\")\n",
    "    print(\"2. 執行全部 (邊跑邊存)\")\n",
    "    mode = input(\"Select: \").strip()\n",
    "\n",
    "    targets = all_folders[:1] if mode == '1' else all_folders\n",
    "\n",
    "    # 批次處理\n",
    "    print(f\"開始處理，結果將即時儲存至: {OUTPUT_CSV}\")\n",
    "    \n",
    "    for folder in tqdm(targets):\n",
    "        path = os.path.abspath(folder)\n",
    "        \n",
    "        # 處理單一公司\n",
    "        res, err = process_single_folder(path, folder, is_test_mode=(mode=='1'))\n",
    "        \n",
    "        # 【立即存檔】\n",
    "        save_chunk(res, OUTPUT_CSV)\n",
    "        save_chunk(err, ERROR_CSV)\n",
    "\n",
    "    print(f\"\\n全數完成！\")\n",
    "    print(f\"結果檔案: {os.path.abspath(OUTPUT_CSV)}\")\n",
    "    if os.path.exists(ERROR_CSV):\n",
    "        print(f\"錯誤紀錄: {os.path.abspath(ERROR_CSV)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d568b50f",
   "metadata": {},
   "source": [
    "- 錯誤重跑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f11e763a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在讀取資料: C:\\Users\\wesley\\OneDrive\\桌面\\LAB\\ai_cup\\company_data_by_annotation_group\\all\\final_corrected_data_incremental.csv\n",
      "發現 89 筆資料需要修復 (強制配對模式)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "修復進度: 100%|██████████| 14/14 [11:04<00:00, 47.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "全部修復完成！\n",
      "輸出檔案: C:\\Users\\wesley\\OneDrive\\桌面\\LAB\\ai_cup\\company_data_by_annotation_group\\all\\final_complete_best_guess.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pdfplumber\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# 忽略 pandas 的未來警告\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# ================= 設定區 =================\n",
    "INPUT_CSV = r\"C:\\Users\\wesley\\OneDrive\\桌面\\LAB\\ai_cup\\company_data_by_annotation_group\\all\\final_corrected_data_incremental.csv\"\n",
    "ROOT_DIR = r\"C:\\Users\\wesley\\OneDrive\\桌面\\LAB\\ai_cup\\company_data_by_annotation_group\\all\"\n",
    "OUTPUT_CSV = r\"C:\\Users\\wesley\\OneDrive\\桌面\\LAB\\ai_cup\\company_data_by_annotation_group\\all\\final_complete_best_guess.csv\"\n",
    "# =========================================\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    修正版：強制轉型為字串，避免 NaN (float) 導致報錯\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # 強制轉成字串\n",
    "    text_str = str(text)\n",
    "    \n",
    "    # 移除所有非中英數的字元\n",
    "    return re.sub(r'[^\\w\\u4e00-\\u9fff]', '', text_str).lower()\n",
    "\n",
    "def get_shingles(text, n=2):\n",
    "    if len(text) < n: return {text}\n",
    "    return set(text[i:i+n] for i in range(len(text) - n + 1))\n",
    "\n",
    "def calculate_hybrid_score(target, page_content):\n",
    "    if not target or not page_content: return 0.0\n",
    "    \n",
    "    # 策略 1: Chunking\n",
    "    chunk_size = 10\n",
    "    chunks = [target[i:i+chunk_size] for i in range(0, len(target), chunk_size) if len(target[i:i+chunk_size]) > 4]\n",
    "    chunk_score = 0.0\n",
    "    if chunks:\n",
    "        hits = sum(1 for c in chunks if c in page_content)\n",
    "        chunk_score = hits / len(chunks)\n",
    "        \n",
    "    if chunk_score > 0.6: return chunk_score\n",
    "\n",
    "    # 策略 2: Shingling\n",
    "    target_shingles = get_shingles(target, n=2)\n",
    "    if not target_shingles: return 0.0\n",
    "    hit_count = sum(1 for s in target_shingles if s in page_content)\n",
    "    return max(chunk_score, hit_count / len(target_shingles))\n",
    "\n",
    "def get_folder_map(root_dir):\n",
    "    folder_map = {}\n",
    "    if not os.path.exists(root_dir): return {}\n",
    "    \n",
    "    dirs = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "    for d in dirs:\n",
    "        match = re.search(r'(\\d{4})', d)\n",
    "        if match:\n",
    "            code = match.group(1)\n",
    "            folder_map[code] = os.path.join(root_dir, d)\n",
    "            folder_map[d] = os.path.join(root_dir, d)\n",
    "    return folder_map\n",
    "\n",
    "def main():\n",
    "    print(f\"正在讀取資料: {INPUT_CSV}\")\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_CSV, encoding='utf-8-sig')\n",
    "    except:\n",
    "        df = pd.read_csv(INPUT_CSV, encoding='utf-8')\n",
    "\n",
    "    # 【重要修正】將所有空值填補為空字串，防止 float error\n",
    "    df = df.fillna(\"\")\n",
    "\n",
    "    # 處理 page_number: 轉數字，無法轉的變 0\n",
    "    df['page_number'] = pd.to_numeric(df['page_number'], errors='coerce').fillna(0)\n",
    "    \n",
    "    # 定義失敗條件：Status 是 Not Found 或者 Page 是 0\n",
    "    mask = (df['_status'].astype(str).str.contains('Not Found')) | (df['page_number'] == 0)\n",
    "    failed_indices = df[mask].index\n",
    "    \n",
    "    if len(failed_indices) == 0:\n",
    "        print(\"恭喜！資料完整，沒有發現需要修復的項目。\")\n",
    "        return\n",
    "\n",
    "    print(f\"發現 {len(failed_indices)} 筆資料需要修復 (強制配對模式)...\")\n",
    "\n",
    "    folder_map = get_folder_map(ROOT_DIR)\n",
    "    \n",
    "    # 取出失敗的子集進行處理\n",
    "    df_failed = df.loc[failed_indices].copy()\n",
    "    grouped = df_failed.groupby('_company_key')\n",
    "\n",
    "    pbar = tqdm(grouped, desc=\"修復進度\")\n",
    "\n",
    "    for company_key, group in pbar:\n",
    "        company_key_str = str(company_key).replace(\".0\", \"\") # 去除可能的浮點數結尾\n",
    "        \n",
    "        target_folder = folder_map.get(company_key_str)\n",
    "        if not target_folder:\n",
    "            # 模糊搜尋\n",
    "            for k, v in folder_map.items():\n",
    "                if company_key_str in k:\n",
    "                    target_folder = v\n",
    "                    break\n",
    "        \n",
    "        if not target_folder:\n",
    "            continue\n",
    "\n",
    "        # 找 PDF\n",
    "        pdf_files = glob.glob(os.path.join(target_folder, \"*.pdf\"))\n",
    "        original_pdf = [f for f in pdf_files if \"_page_\" not in f]\n",
    "        \n",
    "        if not original_pdf:\n",
    "            continue\n",
    "            \n",
    "        pdf_path = original_pdf[0]\n",
    "        folder_name = os.path.basename(target_folder)\n",
    "\n",
    "        # 讀 PDF\n",
    "        page_texts = {}\n",
    "        try:\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                for i, page in enumerate(pdf.pages):\n",
    "                    # 這裡也加強 clean_text 防止 PDF 讀出怪東西\n",
    "                    txt = clean_text(page.extract_text())\n",
    "                    if len(txt) > 5:\n",
    "                        page_texts[i + 1] = txt\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        if not page_texts:\n",
    "            continue\n",
    "\n",
    "        # 針對每一筆失敗資料進行 Best Guess\n",
    "        for idx, row in group.iterrows():\n",
    "            target_text = clean_text(row['data'])\n",
    "            evidence_text = clean_text(row['evidence_string'])\n",
    "\n",
    "            best_page = 1 \n",
    "            best_score = -1.0\n",
    "            \n",
    "            # 強制遍歷所有頁面找最高分\n",
    "            for p_num, p_text in page_texts.items():\n",
    "                s1 = calculate_hybrid_score(target_text, p_text)\n",
    "                \n",
    "                s2 = 0.0\n",
    "                if evidence_text:\n",
    "                    s2 = calculate_hybrid_score(evidence_text, p_text)\n",
    "                \n",
    "                score = max(s1, s2)\n",
    "\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_page = p_num\n",
    "\n",
    "            # 寫回原始 df (使用 loc 更新)\n",
    "            # 即使 best_score 很低，我們也接受 (因為是 Best Guess)\n",
    "            new_url = f\"local_file://{folder_name}_page_{best_page}.pdf\"\n",
    "            \n",
    "            df.loc[idx, 'URL'] = new_url\n",
    "            df.loc[idx, 'page_number'] = int(best_page)\n",
    "            df.loc[idx, '_status'] = \"Repaired\"\n",
    "\n",
    "    # 最終整理\n",
    "    df['page_number'] = df['page_number'].astype(int)\n",
    "    \n",
    "    cols_to_keep = [\n",
    "        \"data\", \"URL\", \"page_number\", \"ESG_type\", \n",
    "        \"promise_status\", \"promise_string\", \"verification_timeline\", \n",
    "        \"evidence_status\", \"evidence_string\", \"evidence_quality\", \n",
    "        \"_company_key\"\n",
    "    ]\n",
    "    \n",
    "    final_cols = [c for c in cols_to_keep if c in df.columns]\n",
    "    df_final = df[final_cols]\n",
    "\n",
    "    print(f\"\\n全部修復完成！\")\n",
    "    print(f\"輸出檔案: {OUTPUT_CSV}\")\n",
    "    df_final.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234db8a4",
   "metadata": {},
   "source": [
    "- 用final_complete_best_guess.csv重新跑pegatron_4938_esg_report_2024.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d7f3832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在讀取 CSV: C:\\Users\\wesley\\OneDrive\\桌面\\LAB\\ai_cup\\company_data_by_annotation_group\\all\\final_complete_best_guess.csv\n",
      "找到 70 筆 Pegatron 資料，準備重跑...\n",
      "正在讀取 PDF: pegatron_4938_esg_report_2024.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "解析 PDF 頁面: 100%|██████████| 57/57 [00:10<00:00,  5.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF 解析完成，開始重新比對資料...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "重跑 Pegatron 資料: 100%|██████████| 70/70 [00:01<00:00, 59.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "重跑完成！共有 54 筆資料的頁碼發生變更。\n",
      "正在儲存至: C:\\Users\\wesley\\OneDrive\\桌面\\LAB\\ai_cup\\company_data_by_annotation_group\\all\\final_complete_pegatron_updated.csv\n",
      "完成。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import pdfplumber\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# 忽略 pandas 警告\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# ================= 設定區 =================\n",
    "# 1. 原始 CSV 路徑\n",
    "INPUT_CSV = r\"C:\\Users\\wesley\\OneDrive\\桌面\\LAB\\ai_cup\\company_data_by_annotation_group\\all\\final_complete_best_guess.csv\"\n",
    "\n",
    "# 2. 根目錄 (用來找 PDF)\n",
    "ROOT_DIR = r\"C:\\Users\\wesley\\OneDrive\\桌面\\LAB\\ai_cup\\company_data_by_annotation_group\\all\"\n",
    "\n",
    "# 3. 指定要重跑的公司資料夾名稱 (必須精確)\n",
    "TARGET_COMPANY_FOLDER = \"pegatron_4938\"\n",
    "TARGET_COMPANY_KEY = 4938  # CSV 裡面的 _company_key 通常是數字\n",
    "\n",
    "# 4. 輸出檔案 (更新後的 CSV)\n",
    "OUTPUT_CSV = r\"C:\\Users\\wesley\\OneDrive\\桌面\\LAB\\ai_cup\\company_data_by_annotation_group\\all\\final_complete_pegatron_updated.csv\"\n",
    "# =========================================\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"強力清洗：防呆(NaN) + 只留中英數\"\"\"\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "    text_str = str(text)\n",
    "    return re.sub(r'[^\\w\\u4e00-\\u9fff]', '', text_str).lower()\n",
    "\n",
    "def get_shingles(text, n=2):\n",
    "    if len(text) < n: return {text}\n",
    "    return set(text[i:i+n] for i in range(len(text) - n + 1))\n",
    "\n",
    "def calculate_segment_score(target, page_content):\n",
    "    \"\"\"分段比對 (針對表格結構 | ●)\"\"\"\n",
    "    segments = re.split(r'[|●•\\n<br>。；;]+', str(target))\n",
    "    valid_segments = [clean_text(s) for s in segments if len(clean_text(s)) > 2]\n",
    "    \n",
    "    if not valid_segments:\n",
    "        return 0.0\n",
    "\n",
    "    hits = sum(1 for seg in valid_segments if seg in page_content)\n",
    "    return hits / len(valid_segments)\n",
    "\n",
    "def calculate_hybrid_score(target, page_content):\n",
    "    \"\"\"綜合比對：取 (長句, 碎片, 分段) 三者最高分\"\"\"\n",
    "    if not target or not page_content: return 0.0\n",
    "    \n",
    "    target = str(target)\n",
    "    \n",
    "    # 1. Chunking (長句)\n",
    "    chunk_size = 10\n",
    "    chunks = [target[i:i+chunk_size] for i in range(0, len(target), chunk_size) if len(target[i:i+chunk_size]) > 4]\n",
    "    chunk_score = 0.0\n",
    "    if chunks:\n",
    "        hits = sum(1 for c in chunks if c in page_content)\n",
    "        chunk_score = hits / len(chunks)\n",
    "\n",
    "    # 2. Shingling (碎片)\n",
    "    target_shingles = get_shingles(clean_text(target), n=2)\n",
    "    shingle_score = 0.0\n",
    "    if target_shingles:\n",
    "        hit_count = sum(1 for s in target_shingles if s in page_content)\n",
    "        shingle_score = hit_count / len(target_shingles)\n",
    "\n",
    "    # 3. Segment (表格分段)\n",
    "    segment_score = calculate_segment_score(target, page_content)\n",
    "\n",
    "    # 回傳最高分\n",
    "    return max(chunk_score, shingle_score, segment_score)\n",
    "\n",
    "def main():\n",
    "    print(f\"正在讀取 CSV: {INPUT_CSV}\")\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_CSV, encoding='utf-8-sig')\n",
    "    except:\n",
    "        df = pd.read_csv(INPUT_CSV, encoding='utf-8')\n",
    "\n",
    "    # 填補空值\n",
    "    df = df.fillna(\"\")\n",
    "\n",
    "    # 篩選出 Pegatron (4938) 的資料\n",
    "    # 注意：有時候 key 是 int, 有時候是 float/string，轉成 string 比對最安全\n",
    "    df['_company_key'] = df['_company_key'].astype(str).str.replace(r'\\.0$', '', regex=True)\n",
    "    target_key_str = str(TARGET_COMPANY_KEY)\n",
    "\n",
    "    pegatron_indices = df[df['_company_key'] == target_key_str].index\n",
    "\n",
    "    if len(pegatron_indices) == 0:\n",
    "        print(f\"錯誤：在 CSV 中找不到公司代號為 {TARGET_COMPANY_KEY} 的資料。\")\n",
    "        print(\"請確認 CSV 中的 _company_key 欄位。\")\n",
    "        return\n",
    "\n",
    "    print(f\"找到 {len(pegatron_indices)} 筆 Pegatron 資料，準備重跑...\")\n",
    "\n",
    "    # 尋找 PDF 路徑\n",
    "    # 路徑結構: .../all/pegatron_4938/xxxxx.pdf\n",
    "    company_dir = os.path.join(ROOT_DIR, TARGET_COMPANY_FOLDER)\n",
    "    if not os.path.exists(company_dir):\n",
    "        print(f\"錯誤：找不到資料夾 {company_dir}\")\n",
    "        return\n",
    "\n",
    "    pdf_files = [f for f in os.listdir(company_dir) if f.endswith(\".pdf\") and \"_page_\" not in f]\n",
    "    if not pdf_files:\n",
    "        print(\"錯誤：資料夾內找不到原始 PDF 檔。\")\n",
    "        return\n",
    "    \n",
    "    pdf_path = os.path.join(company_dir, pdf_files[0])\n",
    "    print(f\"正在讀取 PDF: {pdf_files[0]}\")\n",
    "\n",
    "    # 讀取 PDF 全文索引\n",
    "    page_texts = {}\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for i, page in enumerate(tqdm(pdf.pages, desc=\"解析 PDF 頁面\")):\n",
    "                txt = clean_text(page.extract_text())\n",
    "                if len(txt) > 5:\n",
    "                    page_texts[i + 1] = txt\n",
    "    except Exception as e:\n",
    "        print(f\"PDF 讀取失敗: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"PDF 解析完成，開始重新比對資料...\")\n",
    "\n",
    "    # 開始針對每一筆資料進行重新比對\n",
    "    updated_count = 0\n",
    "    \n",
    "    # 這裡使用 tqdm 顯示進度\n",
    "    for idx in tqdm(pegatron_indices, desc=\"重跑 Pegatron 資料\"):\n",
    "        row = df.loc[idx]\n",
    "        target_text = row['data']\n",
    "        evidence_text = row['evidence_string']\n",
    "\n",
    "        best_page = 1\n",
    "        best_score = -1.0\n",
    "        \n",
    "        # 遍歷每一頁找最高分\n",
    "        for p_num, p_text in page_texts.items():\n",
    "            # 算 Data 分數\n",
    "            s1 = calculate_hybrid_score(target_text, p_text)\n",
    "            \n",
    "            # 算 Evidence 分數\n",
    "            s2 = 0.0\n",
    "            if evidence_text:\n",
    "                s2 = calculate_hybrid_score(evidence_text, p_text)\n",
    "            \n",
    "            # 取兩者最高\n",
    "            score = max(s1, s2)\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_page = p_num\n",
    "\n",
    "        # 更新 DataFrame\n",
    "        new_url = f\"local_file://{TARGET_COMPANY_FOLDER}_page_{best_page}.pdf\"\n",
    "        \n",
    "        # 檢查是否跟原本的不一樣 (可以觀察有多少被修正了)\n",
    "        old_page = df.loc[idx, 'page_number']\n",
    "        try:\n",
    "            old_page = int(float(old_page))\n",
    "        except:\n",
    "            old_page = -1\n",
    "\n",
    "        if old_page != best_page:\n",
    "            updated_count += 1\n",
    "        \n",
    "        df.loc[idx, 'URL'] = new_url\n",
    "        df.loc[idx, 'page_number'] = int(best_page)\n",
    "        # 標記一下狀態，讓你知道這筆是被特定重跑的\n",
    "        df.loc[idx, '_status'] = f\"Updated (Pegatron Re-run, Score: {best_score:.2f})\"\n",
    "\n",
    "    # 輸出\n",
    "    print(f\"\\n重跑完成！共有 {updated_count} 筆資料的頁碼發生變更。\")\n",
    "    \n",
    "    # 整理欄位格式\n",
    "    df['page_number'] = pd.to_numeric(df['page_number'], errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "    cols_to_keep = [\n",
    "        \"data\", \"URL\", \"page_number\", \"ESG_type\", \n",
    "        \"promise_status\", \"promise_string\", \"verification_timeline\", \n",
    "        \"evidence_status\", \"evidence_string\", \"evidence_quality\", \n",
    "        \"_company_key\"\n",
    "    ]\n",
    "    # 保留必要欄位，並加上 _status 方便你看結果 (提交時可移除)\n",
    "    final_cols = [c for c in df.columns if c in cols_to_keep or c == '_status']\n",
    "    \n",
    "    df_final = df[final_cols]\n",
    "    \n",
    "    print(f\"正在儲存至: {OUTPUT_CSV}\")\n",
    "    df_final.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')\n",
    "    print(\"完成。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
